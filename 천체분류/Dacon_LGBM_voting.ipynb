{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dacon_LGBM_voting.ipynb",
      "provenance": [],
      "mount_file_id": "1DoKznfNZz8SqWhZMJLtNm8eoqugYyejO",
      "authorship_tag": "ABX9TyPGoTwutNYWrZ6p7f1Qoh4J"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtOxywXr4QX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "755b20c8-5dff-4993-fd47-a061c0a41c0d"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.stats import norm, skew # 수치해석 기능(확률분포) 제공\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,f1_score,roc_curve,roc_auc_score,log_loss\n",
        "\n",
        "train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/데이콘/천체 유형 분류/train.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/데이콘/천체 유형 분류/test.csv')\n",
        "sample_submission = pd.read_csv('/content/drive/My Drive/Colab Notebooks/data/데이콘/천체 유형 분류/sample_submission.csv')\n",
        "\n",
        "train_id = train['id']\n",
        "test_id = test['id']\n",
        "\n",
        "# 필요없는 Id 컬럼, Drop하자.\n",
        "#train.drop('id',axis=1,inplace=True)\n",
        "#test.drop('id',axis=1,inplace=True)\n",
        "\n",
        "# 'Id'를 제거한 DataFrame의 Shape 확인\n",
        "print(\"The train data size after dropping ID feature is : {} \".format(train.shape))\n",
        "print(\"The test data size after dropping Id feature is : {} \".format(test.shape))\n",
        "\n",
        "unique_labels = train['type'].unique()\n",
        "label_dict = {val: i for i, val in enumerate(unique_labels)}\n",
        "i2lb = {v:k for k, v in label_dict.items()}"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The train data size after dropping ID feature is : (199991, 23) \n",
            "The test data size after dropping Id feature is : (10009, 22) \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2IfyCWQ4Vkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "column_number = {}\n",
        "for i, column in enumerate(sample_submission.columns):\n",
        "    column_number[column] = i\n",
        "    \n",
        "def to_number(x, dic):\n",
        "    return dic[x]\n",
        "\n",
        "train['type_num'] = train['type'].apply(lambda x: to_number(x, column_number))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Are3JaIL4i80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_id=test['id']\n",
        "test = test.drop(columns=['fiberID', 'id'])\n",
        "\n",
        "train_x = train.drop(columns=['id','type_num', 'fiberID','type'], axis=1)\n",
        "train_y = train['type_num']\n",
        "test_x = test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aC9PxwR4mjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_x, train_y, \\\n",
        "                                                  test_size=0.2, random_state=42, stratify = train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGNZ1X5CKcz",
        "colab_type": "text"
      },
      "source": [
        "# LGBM\n",
        "- test logloss: 0.39"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJiAGiAd4u6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #LGBM 1\n",
        "\n",
        "# from lightgbm import LGBMClassifier\n",
        "\n",
        "# lgbm_wrapper = LGBMClassifier(boosting_type='gbdt', num_leaves=250, max_depth=-1, learning_rate=0.005, min_data_in_leaf = 5,\n",
        "#                              n_estimators=10000, subsample_for_bin=250000, objective='multiclass', min_split_gain=0.34, reg_alpha = 0.01, reg_lambda = 0.01,\n",
        "#                              min_child_weight=0.005, min_child_samples=17,n_jobs=-1, random_state=123)\n",
        "\n",
        "# evals =[(X_train,y_train),(X_valid,y_valid)]\n",
        "# lgbm_wrapper.fit(X_train,y_train,early_stopping_rounds=20,\n",
        "#                  eval_metric='logloss',eval_set=evals,verbose=True)\n",
        "\n",
        "# #- test logloss: 0.39"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVj_ZptPXQ8w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6282a171-bed0-4f7b-83d3-963c4bf2c0b9"
      },
      "source": [
        "#LGBM 1\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgbm_wrapper = LGBMClassifier(boosting_type='gbdt', num_leaves=500, max_depth=-1, learning_rate=0.05, min_data_in_leaf = 5,\n",
        "                             n_estimators=10000, subsample_for_bin=250000, objective='multiclass', min_split_gain=0.34, reg_alpha = 0.01, reg_lambda = 0.01,\n",
        "                             min_child_weight=0.005, min_child_samples=17,n_jobs=-1, random_state=123)\n",
        "\n",
        "evals =[(X_train,y_train),(X_valid,y_valid)]\n",
        "lgbm_wrapper.fit(X_train,y_train,early_stopping_rounds=20,\n",
        "                 eval_metric='logloss',eval_set=evals,verbose=True)\n",
        "\n",
        "#- test logloss: 0.39"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\ttraining's multi_logloss: 2.00587\ttraining's multi_logloss: 2.00587\tvalid_1's multi_logloss: 2.04668\tvalid_1's multi_logloss: 2.04668\n",
            "Training until validation scores don't improve for 20 rounds.\n",
            "[2]\ttraining's multi_logloss: 1.83871\ttraining's multi_logloss: 1.83871\tvalid_1's multi_logloss: 1.89375\tvalid_1's multi_logloss: 1.89375\n",
            "[3]\ttraining's multi_logloss: 1.7063\ttraining's multi_logloss: 1.7063\tvalid_1's multi_logloss: 1.77366\tvalid_1's multi_logloss: 1.77366\n",
            "[4]\ttraining's multi_logloss: 1.59472\ttraining's multi_logloss: 1.59472\tvalid_1's multi_logloss: 1.67248\tvalid_1's multi_logloss: 1.67248\n",
            "[5]\ttraining's multi_logloss: 1.49784\ttraining's multi_logloss: 1.49784\tvalid_1's multi_logloss: 1.58428\tvalid_1's multi_logloss: 1.58428\n",
            "[6]\ttraining's multi_logloss: 1.41216\ttraining's multi_logloss: 1.41216\tvalid_1's multi_logloss: 1.50631\tvalid_1's multi_logloss: 1.50631\n",
            "[7]\ttraining's multi_logloss: 1.33544\ttraining's multi_logloss: 1.33544\tvalid_1's multi_logloss: 1.43661\tvalid_1's multi_logloss: 1.43661\n",
            "[8]\ttraining's multi_logloss: 1.26609\ttraining's multi_logloss: 1.26609\tvalid_1's multi_logloss: 1.37361\tvalid_1's multi_logloss: 1.37361\n",
            "[9]\ttraining's multi_logloss: 1.20289\ttraining's multi_logloss: 1.20289\tvalid_1's multi_logloss: 1.31627\tvalid_1's multi_logloss: 1.31627\n",
            "[10]\ttraining's multi_logloss: 1.14491\ttraining's multi_logloss: 1.14491\tvalid_1's multi_logloss: 1.26356\tvalid_1's multi_logloss: 1.26356\n",
            "[11]\ttraining's multi_logloss: 1.09158\ttraining's multi_logloss: 1.09158\tvalid_1's multi_logloss: 1.21511\tvalid_1's multi_logloss: 1.21511\n",
            "[12]\ttraining's multi_logloss: 1.04218\ttraining's multi_logloss: 1.04218\tvalid_1's multi_logloss: 1.17032\tvalid_1's multi_logloss: 1.17032\n",
            "[13]\ttraining's multi_logloss: 0.996286\ttraining's multi_logloss: 0.996286\tvalid_1's multi_logloss: 1.12889\tvalid_1's multi_logloss: 1.12889\n",
            "[14]\ttraining's multi_logloss: 0.953522\ttraining's multi_logloss: 0.953522\tvalid_1's multi_logloss: 1.09031\tvalid_1's multi_logloss: 1.09031\n",
            "[15]\ttraining's multi_logloss: 0.913645\ttraining's multi_logloss: 0.913645\tvalid_1's multi_logloss: 1.05427\tvalid_1's multi_logloss: 1.05427\n",
            "[16]\ttraining's multi_logloss: 0.876193\ttraining's multi_logloss: 0.876193\tvalid_1's multi_logloss: 1.02067\tvalid_1's multi_logloss: 1.02067\n",
            "[17]\ttraining's multi_logloss: 0.840965\ttraining's multi_logloss: 0.840965\tvalid_1's multi_logloss: 0.989213\tvalid_1's multi_logloss: 0.989213\n",
            "[18]\ttraining's multi_logloss: 0.807806\ttraining's multi_logloss: 0.807806\tvalid_1's multi_logloss: 0.959509\tvalid_1's multi_logloss: 0.959509\n",
            "[19]\ttraining's multi_logloss: 0.776693\ttraining's multi_logloss: 0.776693\tvalid_1's multi_logloss: 0.931667\tvalid_1's multi_logloss: 0.931667\n",
            "[20]\ttraining's multi_logloss: 0.747419\ttraining's multi_logloss: 0.747419\tvalid_1's multi_logloss: 0.905464\tvalid_1's multi_logloss: 0.905464\n",
            "[21]\ttraining's multi_logloss: 0.719643\ttraining's multi_logloss: 0.719643\tvalid_1's multi_logloss: 0.880732\tvalid_1's multi_logloss: 0.880732\n",
            "[22]\ttraining's multi_logloss: 0.693335\ttraining's multi_logloss: 0.693335\tvalid_1's multi_logloss: 0.857381\tvalid_1's multi_logloss: 0.857381\n",
            "[23]\ttraining's multi_logloss: 0.668574\ttraining's multi_logloss: 0.668574\tvalid_1's multi_logloss: 0.835526\tvalid_1's multi_logloss: 0.835526\n",
            "[24]\ttraining's multi_logloss: 0.645013\ttraining's multi_logloss: 0.645013\tvalid_1's multi_logloss: 0.814829\tvalid_1's multi_logloss: 0.814829\n",
            "[25]\ttraining's multi_logloss: 0.622697\ttraining's multi_logloss: 0.622697\tvalid_1's multi_logloss: 0.795221\tvalid_1's multi_logloss: 0.795221\n",
            "[26]\ttraining's multi_logloss: 0.601576\ttraining's multi_logloss: 0.601576\tvalid_1's multi_logloss: 0.776716\tvalid_1's multi_logloss: 0.776716\n",
            "[27]\ttraining's multi_logloss: 0.581561\ttraining's multi_logloss: 0.581561\tvalid_1's multi_logloss: 0.759178\tvalid_1's multi_logloss: 0.759178\n",
            "[28]\ttraining's multi_logloss: 0.562513\ttraining's multi_logloss: 0.562513\tvalid_1's multi_logloss: 0.742482\tvalid_1's multi_logloss: 0.742482\n",
            "[29]\ttraining's multi_logloss: 0.544335\ttraining's multi_logloss: 0.544335\tvalid_1's multi_logloss: 0.726677\tvalid_1's multi_logloss: 0.726677\n",
            "[30]\ttraining's multi_logloss: 0.526976\ttraining's multi_logloss: 0.526976\tvalid_1's multi_logloss: 0.711656\tvalid_1's multi_logloss: 0.711656\n",
            "[31]\ttraining's multi_logloss: 0.510496\ttraining's multi_logloss: 0.510496\tvalid_1's multi_logloss: 0.697479\tvalid_1's multi_logloss: 0.697479\n",
            "[32]\ttraining's multi_logloss: 0.494748\ttraining's multi_logloss: 0.494748\tvalid_1's multi_logloss: 0.683971\tvalid_1's multi_logloss: 0.683971\n",
            "[33]\ttraining's multi_logloss: 0.479778\ttraining's multi_logloss: 0.479778\tvalid_1's multi_logloss: 0.671073\tvalid_1's multi_logloss: 0.671073\n",
            "[34]\ttraining's multi_logloss: 0.465461\ttraining's multi_logloss: 0.465461\tvalid_1's multi_logloss: 0.658892\tvalid_1's multi_logloss: 0.658892\n",
            "[35]\ttraining's multi_logloss: 0.451798\ttraining's multi_logloss: 0.451798\tvalid_1's multi_logloss: 0.647335\tvalid_1's multi_logloss: 0.647335\n",
            "[36]\ttraining's multi_logloss: 0.438737\ttraining's multi_logloss: 0.438737\tvalid_1's multi_logloss: 0.636315\tvalid_1's multi_logloss: 0.636315\n",
            "[37]\ttraining's multi_logloss: 0.426208\ttraining's multi_logloss: 0.426208\tvalid_1's multi_logloss: 0.625868\tvalid_1's multi_logloss: 0.625868\n",
            "[38]\ttraining's multi_logloss: 0.414203\ttraining's multi_logloss: 0.414203\tvalid_1's multi_logloss: 0.615883\tvalid_1's multi_logloss: 0.615883\n",
            "[39]\ttraining's multi_logloss: 0.402755\ttraining's multi_logloss: 0.402755\tvalid_1's multi_logloss: 0.606418\tvalid_1's multi_logloss: 0.606418\n",
            "[40]\ttraining's multi_logloss: 0.391761\ttraining's multi_logloss: 0.391761\tvalid_1's multi_logloss: 0.597406\tvalid_1's multi_logloss: 0.597406\n",
            "[41]\ttraining's multi_logloss: 0.381269\ttraining's multi_logloss: 0.381269\tvalid_1's multi_logloss: 0.588891\tvalid_1's multi_logloss: 0.588891\n",
            "[42]\ttraining's multi_logloss: 0.37123\ttraining's multi_logloss: 0.37123\tvalid_1's multi_logloss: 0.580699\tvalid_1's multi_logloss: 0.580699\n",
            "[43]\ttraining's multi_logloss: 0.36167\ttraining's multi_logloss: 0.36167\tvalid_1's multi_logloss: 0.572965\tvalid_1's multi_logloss: 0.572965\n",
            "[44]\ttraining's multi_logloss: 0.352391\ttraining's multi_logloss: 0.352391\tvalid_1's multi_logloss: 0.565599\tvalid_1's multi_logloss: 0.565599\n",
            "[45]\ttraining's multi_logloss: 0.343528\ttraining's multi_logloss: 0.343528\tvalid_1's multi_logloss: 0.558541\tvalid_1's multi_logloss: 0.558541\n",
            "[46]\ttraining's multi_logloss: 0.335072\ttraining's multi_logloss: 0.335072\tvalid_1's multi_logloss: 0.551855\tvalid_1's multi_logloss: 0.551855\n",
            "[47]\ttraining's multi_logloss: 0.326985\ttraining's multi_logloss: 0.326985\tvalid_1's multi_logloss: 0.545512\tvalid_1's multi_logloss: 0.545512\n",
            "[48]\ttraining's multi_logloss: 0.319177\ttraining's multi_logloss: 0.319177\tvalid_1's multi_logloss: 0.539434\tvalid_1's multi_logloss: 0.539434\n",
            "[49]\ttraining's multi_logloss: 0.311728\ttraining's multi_logloss: 0.311728\tvalid_1's multi_logloss: 0.533668\tvalid_1's multi_logloss: 0.533668\n",
            "[50]\ttraining's multi_logloss: 0.304572\ttraining's multi_logloss: 0.304572\tvalid_1's multi_logloss: 0.528159\tvalid_1's multi_logloss: 0.528159\n",
            "[51]\ttraining's multi_logloss: 0.297612\ttraining's multi_logloss: 0.297612\tvalid_1's multi_logloss: 0.522872\tvalid_1's multi_logloss: 0.522872\n",
            "[52]\ttraining's multi_logloss: 0.290879\ttraining's multi_logloss: 0.290879\tvalid_1's multi_logloss: 0.517771\tvalid_1's multi_logloss: 0.517771\n",
            "[53]\ttraining's multi_logloss: 0.284399\ttraining's multi_logloss: 0.284399\tvalid_1's multi_logloss: 0.512912\tvalid_1's multi_logloss: 0.512912\n",
            "[54]\ttraining's multi_logloss: 0.278161\ttraining's multi_logloss: 0.278161\tvalid_1's multi_logloss: 0.508336\tvalid_1's multi_logloss: 0.508336\n",
            "[55]\ttraining's multi_logloss: 0.272199\ttraining's multi_logloss: 0.272199\tvalid_1's multi_logloss: 0.503981\tvalid_1's multi_logloss: 0.503981\n",
            "[56]\ttraining's multi_logloss: 0.266412\ttraining's multi_logloss: 0.266412\tvalid_1's multi_logloss: 0.499806\tvalid_1's multi_logloss: 0.499806\n",
            "[57]\ttraining's multi_logloss: 0.260849\ttraining's multi_logloss: 0.260849\tvalid_1's multi_logloss: 0.4959\tvalid_1's multi_logloss: 0.4959\n",
            "[58]\ttraining's multi_logloss: 0.255485\ttraining's multi_logloss: 0.255485\tvalid_1's multi_logloss: 0.491989\tvalid_1's multi_logloss: 0.491989\n",
            "[59]\ttraining's multi_logloss: 0.25038\ttraining's multi_logloss: 0.25038\tvalid_1's multi_logloss: 0.48831\tvalid_1's multi_logloss: 0.48831\n",
            "[60]\ttraining's multi_logloss: 0.245448\ttraining's multi_logloss: 0.245448\tvalid_1's multi_logloss: 0.484895\tvalid_1's multi_logloss: 0.484895\n",
            "[61]\ttraining's multi_logloss: 0.240745\ttraining's multi_logloss: 0.240745\tvalid_1's multi_logloss: 0.481627\tvalid_1's multi_logloss: 0.481627\n",
            "[62]\ttraining's multi_logloss: 0.236123\ttraining's multi_logloss: 0.236123\tvalid_1's multi_logloss: 0.478478\tvalid_1's multi_logloss: 0.478478\n",
            "[63]\ttraining's multi_logloss: 0.231677\ttraining's multi_logloss: 0.231677\tvalid_1's multi_logloss: 0.47548\tvalid_1's multi_logloss: 0.47548\n",
            "[64]\ttraining's multi_logloss: 0.227384\ttraining's multi_logloss: 0.227384\tvalid_1's multi_logloss: 0.47263\tvalid_1's multi_logloss: 0.47263\n",
            "[65]\ttraining's multi_logloss: 0.223278\ttraining's multi_logloss: 0.223278\tvalid_1's multi_logloss: 0.469898\tvalid_1's multi_logloss: 0.469898\n",
            "[66]\ttraining's multi_logloss: 0.21936\ttraining's multi_logloss: 0.21936\tvalid_1's multi_logloss: 0.467274\tvalid_1's multi_logloss: 0.467274\n",
            "[67]\ttraining's multi_logloss: 0.215566\ttraining's multi_logloss: 0.215566\tvalid_1's multi_logloss: 0.464744\tvalid_1's multi_logloss: 0.464744\n",
            "[68]\ttraining's multi_logloss: 0.211893\ttraining's multi_logloss: 0.211893\tvalid_1's multi_logloss: 0.462301\tvalid_1's multi_logloss: 0.462301\n",
            "[69]\ttraining's multi_logloss: 0.208325\ttraining's multi_logloss: 0.208325\tvalid_1's multi_logloss: 0.460043\tvalid_1's multi_logloss: 0.460043\n",
            "[70]\ttraining's multi_logloss: 0.204836\ttraining's multi_logloss: 0.204836\tvalid_1's multi_logloss: 0.457804\tvalid_1's multi_logloss: 0.457804\n",
            "[71]\ttraining's multi_logloss: 0.201525\ttraining's multi_logloss: 0.201525\tvalid_1's multi_logloss: 0.455642\tvalid_1's multi_logloss: 0.455642\n",
            "[72]\ttraining's multi_logloss: 0.198307\ttraining's multi_logloss: 0.198307\tvalid_1's multi_logloss: 0.453566\tvalid_1's multi_logloss: 0.453566\n",
            "[73]\ttraining's multi_logloss: 0.195143\ttraining's multi_logloss: 0.195143\tvalid_1's multi_logloss: 0.451594\tvalid_1's multi_logloss: 0.451594\n",
            "[74]\ttraining's multi_logloss: 0.19214\ttraining's multi_logloss: 0.19214\tvalid_1's multi_logloss: 0.4498\tvalid_1's multi_logloss: 0.4498\n",
            "[75]\ttraining's multi_logloss: 0.189213\ttraining's multi_logloss: 0.189213\tvalid_1's multi_logloss: 0.448024\tvalid_1's multi_logloss: 0.448024\n",
            "[76]\ttraining's multi_logloss: 0.186462\ttraining's multi_logloss: 0.186462\tvalid_1's multi_logloss: 0.446316\tvalid_1's multi_logloss: 0.446316\n",
            "[77]\ttraining's multi_logloss: 0.183812\ttraining's multi_logloss: 0.183812\tvalid_1's multi_logloss: 0.444658\tvalid_1's multi_logloss: 0.444658\n",
            "[78]\ttraining's multi_logloss: 0.181199\ttraining's multi_logloss: 0.181199\tvalid_1's multi_logloss: 0.443071\tvalid_1's multi_logloss: 0.443071\n",
            "[79]\ttraining's multi_logloss: 0.178723\ttraining's multi_logloss: 0.178723\tvalid_1's multi_logloss: 0.441567\tvalid_1's multi_logloss: 0.441567\n",
            "[80]\ttraining's multi_logloss: 0.176259\ttraining's multi_logloss: 0.176259\tvalid_1's multi_logloss: 0.440115\tvalid_1's multi_logloss: 0.440115\n",
            "[81]\ttraining's multi_logloss: 0.1739\ttraining's multi_logloss: 0.1739\tvalid_1's multi_logloss: 0.438669\tvalid_1's multi_logloss: 0.438669\n",
            "[82]\ttraining's multi_logloss: 0.171634\ttraining's multi_logloss: 0.171634\tvalid_1's multi_logloss: 0.437398\tvalid_1's multi_logloss: 0.437398\n",
            "[83]\ttraining's multi_logloss: 0.169441\ttraining's multi_logloss: 0.169441\tvalid_1's multi_logloss: 0.436137\tvalid_1's multi_logloss: 0.436137\n",
            "[84]\ttraining's multi_logloss: 0.167315\ttraining's multi_logloss: 0.167315\tvalid_1's multi_logloss: 0.434931\tvalid_1's multi_logloss: 0.434931\n",
            "[85]\ttraining's multi_logloss: 0.165288\ttraining's multi_logloss: 0.165288\tvalid_1's multi_logloss: 0.43382\tvalid_1's multi_logloss: 0.43382\n",
            "[86]\ttraining's multi_logloss: 0.163306\ttraining's multi_logloss: 0.163306\tvalid_1's multi_logloss: 0.432689\tvalid_1's multi_logloss: 0.432689\n",
            "[87]\ttraining's multi_logloss: 0.161422\ttraining's multi_logloss: 0.161422\tvalid_1's multi_logloss: 0.431641\tvalid_1's multi_logloss: 0.431641\n",
            "[88]\ttraining's multi_logloss: 0.159558\ttraining's multi_logloss: 0.159558\tvalid_1's multi_logloss: 0.430632\tvalid_1's multi_logloss: 0.430632\n",
            "[89]\ttraining's multi_logloss: 0.157739\ttraining's multi_logloss: 0.157739\tvalid_1's multi_logloss: 0.429565\tvalid_1's multi_logloss: 0.429565\n",
            "[90]\ttraining's multi_logloss: 0.156067\ttraining's multi_logloss: 0.156067\tvalid_1's multi_logloss: 0.428618\tvalid_1's multi_logloss: 0.428618\n",
            "[91]\ttraining's multi_logloss: 0.154366\ttraining's multi_logloss: 0.154366\tvalid_1's multi_logloss: 0.427752\tvalid_1's multi_logloss: 0.427752\n",
            "[92]\ttraining's multi_logloss: 0.152721\ttraining's multi_logloss: 0.152721\tvalid_1's multi_logloss: 0.426883\tvalid_1's multi_logloss: 0.426883\n",
            "[93]\ttraining's multi_logloss: 0.151179\ttraining's multi_logloss: 0.151179\tvalid_1's multi_logloss: 0.426066\tvalid_1's multi_logloss: 0.426066\n",
            "[94]\ttraining's multi_logloss: 0.149682\ttraining's multi_logloss: 0.149682\tvalid_1's multi_logloss: 0.425266\tvalid_1's multi_logloss: 0.425266\n",
            "[95]\ttraining's multi_logloss: 0.148204\ttraining's multi_logloss: 0.148204\tvalid_1's multi_logloss: 0.424452\tvalid_1's multi_logloss: 0.424452\n",
            "[96]\ttraining's multi_logloss: 0.146783\ttraining's multi_logloss: 0.146783\tvalid_1's multi_logloss: 0.423714\tvalid_1's multi_logloss: 0.423714\n",
            "[97]\ttraining's multi_logloss: 0.145377\ttraining's multi_logloss: 0.145377\tvalid_1's multi_logloss: 0.423011\tvalid_1's multi_logloss: 0.423011\n",
            "[98]\ttraining's multi_logloss: 0.144019\ttraining's multi_logloss: 0.144019\tvalid_1's multi_logloss: 0.422326\tvalid_1's multi_logloss: 0.422326\n",
            "[99]\ttraining's multi_logloss: 0.142737\ttraining's multi_logloss: 0.142737\tvalid_1's multi_logloss: 0.421727\tvalid_1's multi_logloss: 0.421727\n",
            "[100]\ttraining's multi_logloss: 0.141462\ttraining's multi_logloss: 0.141462\tvalid_1's multi_logloss: 0.421107\tvalid_1's multi_logloss: 0.421107\n",
            "[101]\ttraining's multi_logloss: 0.140304\ttraining's multi_logloss: 0.140304\tvalid_1's multi_logloss: 0.42048\tvalid_1's multi_logloss: 0.42048\n",
            "[102]\ttraining's multi_logloss: 0.139182\ttraining's multi_logloss: 0.139182\tvalid_1's multi_logloss: 0.419883\tvalid_1's multi_logloss: 0.419883\n",
            "[103]\ttraining's multi_logloss: 0.137987\ttraining's multi_logloss: 0.137987\tvalid_1's multi_logloss: 0.419372\tvalid_1's multi_logloss: 0.419372\n",
            "[104]\ttraining's multi_logloss: 0.136861\ttraining's multi_logloss: 0.136861\tvalid_1's multi_logloss: 0.418839\tvalid_1's multi_logloss: 0.418839\n",
            "[105]\ttraining's multi_logloss: 0.135806\ttraining's multi_logloss: 0.135806\tvalid_1's multi_logloss: 0.418328\tvalid_1's multi_logloss: 0.418328\n",
            "[106]\ttraining's multi_logloss: 0.134859\ttraining's multi_logloss: 0.134859\tvalid_1's multi_logloss: 0.417826\tvalid_1's multi_logloss: 0.417826\n",
            "[107]\ttraining's multi_logloss: 0.133889\ttraining's multi_logloss: 0.133889\tvalid_1's multi_logloss: 0.417322\tvalid_1's multi_logloss: 0.417322\n",
            "[108]\ttraining's multi_logloss: 0.132914\ttraining's multi_logloss: 0.132914\tvalid_1's multi_logloss: 0.416881\tvalid_1's multi_logloss: 0.416881\n",
            "[109]\ttraining's multi_logloss: 0.131983\ttraining's multi_logloss: 0.131983\tvalid_1's multi_logloss: 0.416448\tvalid_1's multi_logloss: 0.416448\n",
            "[110]\ttraining's multi_logloss: 0.131063\ttraining's multi_logloss: 0.131063\tvalid_1's multi_logloss: 0.41604\tvalid_1's multi_logloss: 0.41604\n",
            "[111]\ttraining's multi_logloss: 0.130194\ttraining's multi_logloss: 0.130194\tvalid_1's multi_logloss: 0.415591\tvalid_1's multi_logloss: 0.415591\n",
            "[112]\ttraining's multi_logloss: 0.129383\ttraining's multi_logloss: 0.129383\tvalid_1's multi_logloss: 0.415186\tvalid_1's multi_logloss: 0.415186\n",
            "[113]\ttraining's multi_logloss: 0.128632\ttraining's multi_logloss: 0.128632\tvalid_1's multi_logloss: 0.414808\tvalid_1's multi_logloss: 0.414808\n",
            "[114]\ttraining's multi_logloss: 0.12791\ttraining's multi_logloss: 0.12791\tvalid_1's multi_logloss: 0.414445\tvalid_1's multi_logloss: 0.414445\n",
            "[115]\ttraining's multi_logloss: 0.127236\ttraining's multi_logloss: 0.127236\tvalid_1's multi_logloss: 0.414106\tvalid_1's multi_logloss: 0.414106\n",
            "[116]\ttraining's multi_logloss: 0.126576\ttraining's multi_logloss: 0.126576\tvalid_1's multi_logloss: 0.413787\tvalid_1's multi_logloss: 0.413787\n",
            "[117]\ttraining's multi_logloss: 0.125976\ttraining's multi_logloss: 0.125976\tvalid_1's multi_logloss: 0.413484\tvalid_1's multi_logloss: 0.413484\n",
            "[118]\ttraining's multi_logloss: 0.125352\ttraining's multi_logloss: 0.125352\tvalid_1's multi_logloss: 0.413187\tvalid_1's multi_logloss: 0.413187\n",
            "[119]\ttraining's multi_logloss: 0.124827\ttraining's multi_logloss: 0.124827\tvalid_1's multi_logloss: 0.412894\tvalid_1's multi_logloss: 0.412894\n",
            "[120]\ttraining's multi_logloss: 0.12427\ttraining's multi_logloss: 0.12427\tvalid_1's multi_logloss: 0.412584\tvalid_1's multi_logloss: 0.412584\n",
            "[121]\ttraining's multi_logloss: 0.123626\ttraining's multi_logloss: 0.123626\tvalid_1's multi_logloss: 0.412299\tvalid_1's multi_logloss: 0.412299\n",
            "[122]\ttraining's multi_logloss: 0.123133\ttraining's multi_logloss: 0.123133\tvalid_1's multi_logloss: 0.41204\tvalid_1's multi_logloss: 0.41204\n",
            "[123]\ttraining's multi_logloss: 0.122654\ttraining's multi_logloss: 0.122654\tvalid_1's multi_logloss: 0.41176\tvalid_1's multi_logloss: 0.41176\n",
            "[124]\ttraining's multi_logloss: 0.122223\ttraining's multi_logloss: 0.122223\tvalid_1's multi_logloss: 0.411512\tvalid_1's multi_logloss: 0.411512\n",
            "[125]\ttraining's multi_logloss: 0.1217\ttraining's multi_logloss: 0.1217\tvalid_1's multi_logloss: 0.411282\tvalid_1's multi_logloss: 0.411282\n",
            "[126]\ttraining's multi_logloss: 0.121219\ttraining's multi_logloss: 0.121219\tvalid_1's multi_logloss: 0.411037\tvalid_1's multi_logloss: 0.411037\n",
            "[127]\ttraining's multi_logloss: 0.120833\ttraining's multi_logloss: 0.120833\tvalid_1's multi_logloss: 0.410814\tvalid_1's multi_logloss: 0.410814\n",
            "[128]\ttraining's multi_logloss: 0.120447\ttraining's multi_logloss: 0.120447\tvalid_1's multi_logloss: 0.410633\tvalid_1's multi_logloss: 0.410633\n",
            "[129]\ttraining's multi_logloss: 0.120134\ttraining's multi_logloss: 0.120134\tvalid_1's multi_logloss: 0.410412\tvalid_1's multi_logloss: 0.410412\n",
            "[130]\ttraining's multi_logloss: 0.119708\ttraining's multi_logloss: 0.119708\tvalid_1's multi_logloss: 0.410209\tvalid_1's multi_logloss: 0.410209\n",
            "[131]\ttraining's multi_logloss: 0.119321\ttraining's multi_logloss: 0.119321\tvalid_1's multi_logloss: 0.409993\tvalid_1's multi_logloss: 0.409993\n",
            "[132]\ttraining's multi_logloss: 0.11898\ttraining's multi_logloss: 0.11898\tvalid_1's multi_logloss: 0.409796\tvalid_1's multi_logloss: 0.409796\n",
            "[133]\ttraining's multi_logloss: 0.118691\ttraining's multi_logloss: 0.118691\tvalid_1's multi_logloss: 0.409608\tvalid_1's multi_logloss: 0.409608\n",
            "[134]\ttraining's multi_logloss: 0.118377\ttraining's multi_logloss: 0.118377\tvalid_1's multi_logloss: 0.409428\tvalid_1's multi_logloss: 0.409428\n",
            "[135]\ttraining's multi_logloss: 0.118048\ttraining's multi_logloss: 0.118048\tvalid_1's multi_logloss: 0.409246\tvalid_1's multi_logloss: 0.409246\n",
            "[136]\ttraining's multi_logloss: 0.117676\ttraining's multi_logloss: 0.117676\tvalid_1's multi_logloss: 0.409105\tvalid_1's multi_logloss: 0.409105\n",
            "[137]\ttraining's multi_logloss: 0.117403\ttraining's multi_logloss: 0.117403\tvalid_1's multi_logloss: 0.408961\tvalid_1's multi_logloss: 0.408961\n",
            "[138]\ttraining's multi_logloss: 0.117152\ttraining's multi_logloss: 0.117152\tvalid_1's multi_logloss: 0.40882\tvalid_1's multi_logloss: 0.40882\n",
            "[139]\ttraining's multi_logloss: 0.116842\ttraining's multi_logloss: 0.116842\tvalid_1's multi_logloss: 0.408667\tvalid_1's multi_logloss: 0.408667\n",
            "[140]\ttraining's multi_logloss: 0.116626\ttraining's multi_logloss: 0.116626\tvalid_1's multi_logloss: 0.408519\tvalid_1's multi_logloss: 0.408519\n",
            "[141]\ttraining's multi_logloss: 0.116411\ttraining's multi_logloss: 0.116411\tvalid_1's multi_logloss: 0.408396\tvalid_1's multi_logloss: 0.408396\n",
            "[142]\ttraining's multi_logloss: 0.116197\ttraining's multi_logloss: 0.116197\tvalid_1's multi_logloss: 0.408252\tvalid_1's multi_logloss: 0.408252\n",
            "[143]\ttraining's multi_logloss: 0.116006\ttraining's multi_logloss: 0.116006\tvalid_1's multi_logloss: 0.408104\tvalid_1's multi_logloss: 0.408104\n",
            "[144]\ttraining's multi_logloss: 0.115819\ttraining's multi_logloss: 0.115819\tvalid_1's multi_logloss: 0.407965\tvalid_1's multi_logloss: 0.407965\n",
            "[145]\ttraining's multi_logloss: 0.115626\ttraining's multi_logloss: 0.115626\tvalid_1's multi_logloss: 0.407844\tvalid_1's multi_logloss: 0.407844\n",
            "[146]\ttraining's multi_logloss: 0.115434\ttraining's multi_logloss: 0.115434\tvalid_1's multi_logloss: 0.407715\tvalid_1's multi_logloss: 0.407715\n",
            "[147]\ttraining's multi_logloss: 0.115263\ttraining's multi_logloss: 0.115263\tvalid_1's multi_logloss: 0.4076\tvalid_1's multi_logloss: 0.4076\n",
            "[148]\ttraining's multi_logloss: 0.115045\ttraining's multi_logloss: 0.115045\tvalid_1's multi_logloss: 0.407487\tvalid_1's multi_logloss: 0.407487\n",
            "[149]\ttraining's multi_logloss: 0.114888\ttraining's multi_logloss: 0.114888\tvalid_1's multi_logloss: 0.407381\tvalid_1's multi_logloss: 0.407381\n",
            "[150]\ttraining's multi_logloss: 0.114675\ttraining's multi_logloss: 0.114675\tvalid_1's multi_logloss: 0.407269\tvalid_1's multi_logloss: 0.407269\n",
            "[151]\ttraining's multi_logloss: 0.114531\ttraining's multi_logloss: 0.114531\tvalid_1's multi_logloss: 0.407165\tvalid_1's multi_logloss: 0.407165\n",
            "[152]\ttraining's multi_logloss: 0.114382\ttraining's multi_logloss: 0.114382\tvalid_1's multi_logloss: 0.407062\tvalid_1's multi_logloss: 0.407062\n",
            "[153]\ttraining's multi_logloss: 0.114244\ttraining's multi_logloss: 0.114244\tvalid_1's multi_logloss: 0.406982\tvalid_1's multi_logloss: 0.406982\n",
            "[154]\ttraining's multi_logloss: 0.11407\ttraining's multi_logloss: 0.11407\tvalid_1's multi_logloss: 0.406909\tvalid_1's multi_logloss: 0.406909\n",
            "[155]\ttraining's multi_logloss: 0.113954\ttraining's multi_logloss: 0.113954\tvalid_1's multi_logloss: 0.406818\tvalid_1's multi_logloss: 0.406818\n",
            "[156]\ttraining's multi_logloss: 0.113808\ttraining's multi_logloss: 0.113808\tvalid_1's multi_logloss: 0.406746\tvalid_1's multi_logloss: 0.406746\n",
            "[157]\ttraining's multi_logloss: 0.113677\ttraining's multi_logloss: 0.113677\tvalid_1's multi_logloss: 0.406671\tvalid_1's multi_logloss: 0.406671\n",
            "[158]\ttraining's multi_logloss: 0.113569\ttraining's multi_logloss: 0.113569\tvalid_1's multi_logloss: 0.406603\tvalid_1's multi_logloss: 0.406603\n",
            "[159]\ttraining's multi_logloss: 0.113446\ttraining's multi_logloss: 0.113446\tvalid_1's multi_logloss: 0.40654\tvalid_1's multi_logloss: 0.40654\n",
            "[160]\ttraining's multi_logloss: 0.113323\ttraining's multi_logloss: 0.113323\tvalid_1's multi_logloss: 0.406473\tvalid_1's multi_logloss: 0.406473\n",
            "[161]\ttraining's multi_logloss: 0.113166\ttraining's multi_logloss: 0.113166\tvalid_1's multi_logloss: 0.406404\tvalid_1's multi_logloss: 0.406404\n",
            "[162]\ttraining's multi_logloss: 0.113047\ttraining's multi_logloss: 0.113047\tvalid_1's multi_logloss: 0.40634\tvalid_1's multi_logloss: 0.40634\n",
            "[163]\ttraining's multi_logloss: 0.11289\ttraining's multi_logloss: 0.11289\tvalid_1's multi_logloss: 0.406288\tvalid_1's multi_logloss: 0.406288\n",
            "[164]\ttraining's multi_logloss: 0.112807\ttraining's multi_logloss: 0.112807\tvalid_1's multi_logloss: 0.406223\tvalid_1's multi_logloss: 0.406223\n",
            "[165]\ttraining's multi_logloss: 0.112716\ttraining's multi_logloss: 0.112716\tvalid_1's multi_logloss: 0.406171\tvalid_1's multi_logloss: 0.406171\n",
            "[166]\ttraining's multi_logloss: 0.112626\ttraining's multi_logloss: 0.112626\tvalid_1's multi_logloss: 0.406117\tvalid_1's multi_logloss: 0.406117\n",
            "[167]\ttraining's multi_logloss: 0.112539\ttraining's multi_logloss: 0.112539\tvalid_1's multi_logloss: 0.406075\tvalid_1's multi_logloss: 0.406075\n",
            "[168]\ttraining's multi_logloss: 0.112466\ttraining's multi_logloss: 0.112466\tvalid_1's multi_logloss: 0.406024\tvalid_1's multi_logloss: 0.406024\n",
            "[169]\ttraining's multi_logloss: 0.112379\ttraining's multi_logloss: 0.112379\tvalid_1's multi_logloss: 0.405972\tvalid_1's multi_logloss: 0.405972\n",
            "[170]\ttraining's multi_logloss: 0.112298\ttraining's multi_logloss: 0.112298\tvalid_1's multi_logloss: 0.405919\tvalid_1's multi_logloss: 0.405919\n",
            "[171]\ttraining's multi_logloss: 0.11221\ttraining's multi_logloss: 0.11221\tvalid_1's multi_logloss: 0.405873\tvalid_1's multi_logloss: 0.405873\n",
            "[172]\ttraining's multi_logloss: 0.112143\ttraining's multi_logloss: 0.112143\tvalid_1's multi_logloss: 0.40583\tvalid_1's multi_logloss: 0.40583\n",
            "[173]\ttraining's multi_logloss: 0.112081\ttraining's multi_logloss: 0.112081\tvalid_1's multi_logloss: 0.405787\tvalid_1's multi_logloss: 0.405787\n",
            "[174]\ttraining's multi_logloss: 0.112024\ttraining's multi_logloss: 0.112024\tvalid_1's multi_logloss: 0.405748\tvalid_1's multi_logloss: 0.405748\n",
            "[175]\ttraining's multi_logloss: 0.111973\ttraining's multi_logloss: 0.111973\tvalid_1's multi_logloss: 0.405711\tvalid_1's multi_logloss: 0.405711\n",
            "[176]\ttraining's multi_logloss: 0.11192\ttraining's multi_logloss: 0.11192\tvalid_1's multi_logloss: 0.405676\tvalid_1's multi_logloss: 0.405676\n",
            "[177]\ttraining's multi_logloss: 0.111861\ttraining's multi_logloss: 0.111861\tvalid_1's multi_logloss: 0.405647\tvalid_1's multi_logloss: 0.405647\n",
            "[178]\ttraining's multi_logloss: 0.111809\ttraining's multi_logloss: 0.111809\tvalid_1's multi_logloss: 0.405611\tvalid_1's multi_logloss: 0.405611\n",
            "[179]\ttraining's multi_logloss: 0.111751\ttraining's multi_logloss: 0.111751\tvalid_1's multi_logloss: 0.405573\tvalid_1's multi_logloss: 0.405573\n",
            "[180]\ttraining's multi_logloss: 0.111703\ttraining's multi_logloss: 0.111703\tvalid_1's multi_logloss: 0.40554\tvalid_1's multi_logloss: 0.40554\n",
            "[181]\ttraining's multi_logloss: 0.111657\ttraining's multi_logloss: 0.111657\tvalid_1's multi_logloss: 0.40551\tvalid_1's multi_logloss: 0.40551\n",
            "[182]\ttraining's multi_logloss: 0.111608\ttraining's multi_logloss: 0.111608\tvalid_1's multi_logloss: 0.405479\tvalid_1's multi_logloss: 0.405479\n",
            "[183]\ttraining's multi_logloss: 0.111547\ttraining's multi_logloss: 0.111547\tvalid_1's multi_logloss: 0.405452\tvalid_1's multi_logloss: 0.405452\n",
            "[184]\ttraining's multi_logloss: 0.111479\ttraining's multi_logloss: 0.111479\tvalid_1's multi_logloss: 0.40543\tvalid_1's multi_logloss: 0.40543\n",
            "[185]\ttraining's multi_logloss: 0.11143\ttraining's multi_logloss: 0.11143\tvalid_1's multi_logloss: 0.405405\tvalid_1's multi_logloss: 0.405405\n",
            "[186]\ttraining's multi_logloss: 0.111315\ttraining's multi_logloss: 0.111315\tvalid_1's multi_logloss: 0.405378\tvalid_1's multi_logloss: 0.405378\n",
            "[187]\ttraining's multi_logloss: 0.111254\ttraining's multi_logloss: 0.111254\tvalid_1's multi_logloss: 0.405356\tvalid_1's multi_logloss: 0.405356\n",
            "[188]\ttraining's multi_logloss: 0.111144\ttraining's multi_logloss: 0.111144\tvalid_1's multi_logloss: 0.405324\tvalid_1's multi_logloss: 0.405324\n",
            "[189]\ttraining's multi_logloss: 0.111101\ttraining's multi_logloss: 0.111101\tvalid_1's multi_logloss: 0.405296\tvalid_1's multi_logloss: 0.405296\n",
            "[190]\ttraining's multi_logloss: 0.111064\ttraining's multi_logloss: 0.111064\tvalid_1's multi_logloss: 0.405277\tvalid_1's multi_logloss: 0.405277\n",
            "[191]\ttraining's multi_logloss: 0.110998\ttraining's multi_logloss: 0.110998\tvalid_1's multi_logloss: 0.405252\tvalid_1's multi_logloss: 0.405252\n",
            "[192]\ttraining's multi_logloss: 0.110963\ttraining's multi_logloss: 0.110963\tvalid_1's multi_logloss: 0.405234\tvalid_1's multi_logloss: 0.405234\n",
            "[193]\ttraining's multi_logloss: 0.110897\ttraining's multi_logloss: 0.110897\tvalid_1's multi_logloss: 0.405217\tvalid_1's multi_logloss: 0.405217\n",
            "[194]\ttraining's multi_logloss: 0.110866\ttraining's multi_logloss: 0.110866\tvalid_1's multi_logloss: 0.405197\tvalid_1's multi_logloss: 0.405197\n",
            "[195]\ttraining's multi_logloss: 0.11083\ttraining's multi_logloss: 0.11083\tvalid_1's multi_logloss: 0.405186\tvalid_1's multi_logloss: 0.405186\n",
            "[196]\ttraining's multi_logloss: 0.110789\ttraining's multi_logloss: 0.110789\tvalid_1's multi_logloss: 0.405165\tvalid_1's multi_logloss: 0.405165\n",
            "[197]\ttraining's multi_logloss: 0.11075\ttraining's multi_logloss: 0.11075\tvalid_1's multi_logloss: 0.405148\tvalid_1's multi_logloss: 0.405148\n",
            "[198]\ttraining's multi_logloss: 0.110713\ttraining's multi_logloss: 0.110713\tvalid_1's multi_logloss: 0.40514\tvalid_1's multi_logloss: 0.40514\n",
            "[199]\ttraining's multi_logloss: 0.110683\ttraining's multi_logloss: 0.110683\tvalid_1's multi_logloss: 0.405119\tvalid_1's multi_logloss: 0.405119\n",
            "[200]\ttraining's multi_logloss: 0.110656\ttraining's multi_logloss: 0.110656\tvalid_1's multi_logloss: 0.405102\tvalid_1's multi_logloss: 0.405102\n",
            "[201]\ttraining's multi_logloss: 0.110622\ttraining's multi_logloss: 0.110622\tvalid_1's multi_logloss: 0.405091\tvalid_1's multi_logloss: 0.405091\n",
            "[202]\ttraining's multi_logloss: 0.110597\ttraining's multi_logloss: 0.110597\tvalid_1's multi_logloss: 0.405068\tvalid_1's multi_logloss: 0.405068\n",
            "[203]\ttraining's multi_logloss: 0.110572\ttraining's multi_logloss: 0.110572\tvalid_1's multi_logloss: 0.405052\tvalid_1's multi_logloss: 0.405052\n",
            "[204]\ttraining's multi_logloss: 0.110547\ttraining's multi_logloss: 0.110547\tvalid_1's multi_logloss: 0.405035\tvalid_1's multi_logloss: 0.405035\n",
            "[205]\ttraining's multi_logloss: 0.11052\ttraining's multi_logloss: 0.11052\tvalid_1's multi_logloss: 0.405018\tvalid_1's multi_logloss: 0.405018\n",
            "[206]\ttraining's multi_logloss: 0.110498\ttraining's multi_logloss: 0.110498\tvalid_1's multi_logloss: 0.405003\tvalid_1's multi_logloss: 0.405003\n",
            "[207]\ttraining's multi_logloss: 0.110474\ttraining's multi_logloss: 0.110474\tvalid_1's multi_logloss: 0.404995\tvalid_1's multi_logloss: 0.404995\n",
            "[208]\ttraining's multi_logloss: 0.110455\ttraining's multi_logloss: 0.110455\tvalid_1's multi_logloss: 0.404984\tvalid_1's multi_logloss: 0.404984\n",
            "[209]\ttraining's multi_logloss: 0.110433\ttraining's multi_logloss: 0.110433\tvalid_1's multi_logloss: 0.404969\tvalid_1's multi_logloss: 0.404969\n",
            "[210]\ttraining's multi_logloss: 0.110409\ttraining's multi_logloss: 0.110409\tvalid_1's multi_logloss: 0.404958\tvalid_1's multi_logloss: 0.404958\n",
            "[211]\ttraining's multi_logloss: 0.110388\ttraining's multi_logloss: 0.110388\tvalid_1's multi_logloss: 0.404946\tvalid_1's multi_logloss: 0.404946\n",
            "[212]\ttraining's multi_logloss: 0.110367\ttraining's multi_logloss: 0.110367\tvalid_1's multi_logloss: 0.404937\tvalid_1's multi_logloss: 0.404937\n",
            "[213]\ttraining's multi_logloss: 0.110349\ttraining's multi_logloss: 0.110349\tvalid_1's multi_logloss: 0.404926\tvalid_1's multi_logloss: 0.404926\n",
            "[214]\ttraining's multi_logloss: 0.110333\ttraining's multi_logloss: 0.110333\tvalid_1's multi_logloss: 0.404915\tvalid_1's multi_logloss: 0.404915\n",
            "[215]\ttraining's multi_logloss: 0.110318\ttraining's multi_logloss: 0.110318\tvalid_1's multi_logloss: 0.404906\tvalid_1's multi_logloss: 0.404906\n",
            "[216]\ttraining's multi_logloss: 0.110294\ttraining's multi_logloss: 0.110294\tvalid_1's multi_logloss: 0.4049\tvalid_1's multi_logloss: 0.4049\n",
            "[217]\ttraining's multi_logloss: 0.110278\ttraining's multi_logloss: 0.110278\tvalid_1's multi_logloss: 0.40489\tvalid_1's multi_logloss: 0.40489\n",
            "[218]\ttraining's multi_logloss: 0.110261\ttraining's multi_logloss: 0.110261\tvalid_1's multi_logloss: 0.404874\tvalid_1's multi_logloss: 0.404874\n",
            "[219]\ttraining's multi_logloss: 0.110245\ttraining's multi_logloss: 0.110245\tvalid_1's multi_logloss: 0.404868\tvalid_1's multi_logloss: 0.404868\n",
            "[220]\ttraining's multi_logloss: 0.11023\ttraining's multi_logloss: 0.11023\tvalid_1's multi_logloss: 0.404858\tvalid_1's multi_logloss: 0.404858\n",
            "[221]\ttraining's multi_logloss: 0.110217\ttraining's multi_logloss: 0.110217\tvalid_1's multi_logloss: 0.404852\tvalid_1's multi_logloss: 0.404852\n",
            "[222]\ttraining's multi_logloss: 0.110204\ttraining's multi_logloss: 0.110204\tvalid_1's multi_logloss: 0.404845\tvalid_1's multi_logloss: 0.404845\n",
            "[223]\ttraining's multi_logloss: 0.110191\ttraining's multi_logloss: 0.110191\tvalid_1's multi_logloss: 0.404831\tvalid_1's multi_logloss: 0.404831\n",
            "[224]\ttraining's multi_logloss: 0.110178\ttraining's multi_logloss: 0.110178\tvalid_1's multi_logloss: 0.404822\tvalid_1's multi_logloss: 0.404822\n",
            "[225]\ttraining's multi_logloss: 0.110166\ttraining's multi_logloss: 0.110166\tvalid_1's multi_logloss: 0.404813\tvalid_1's multi_logloss: 0.404813\n",
            "[226]\ttraining's multi_logloss: 0.110153\ttraining's multi_logloss: 0.110153\tvalid_1's multi_logloss: 0.40481\tvalid_1's multi_logloss: 0.40481\n",
            "[227]\ttraining's multi_logloss: 0.110143\ttraining's multi_logloss: 0.110143\tvalid_1's multi_logloss: 0.404804\tvalid_1's multi_logloss: 0.404804\n",
            "[228]\ttraining's multi_logloss: 0.110132\ttraining's multi_logloss: 0.110132\tvalid_1's multi_logloss: 0.404794\tvalid_1's multi_logloss: 0.404794\n",
            "[229]\ttraining's multi_logloss: 0.110123\ttraining's multi_logloss: 0.110123\tvalid_1's multi_logloss: 0.404785\tvalid_1's multi_logloss: 0.404785\n",
            "[230]\ttraining's multi_logloss: 0.11011\ttraining's multi_logloss: 0.11011\tvalid_1's multi_logloss: 0.404775\tvalid_1's multi_logloss: 0.404775\n",
            "[231]\ttraining's multi_logloss: 0.110096\ttraining's multi_logloss: 0.110096\tvalid_1's multi_logloss: 0.404768\tvalid_1's multi_logloss: 0.404768\n",
            "[232]\ttraining's multi_logloss: 0.110086\ttraining's multi_logloss: 0.110086\tvalid_1's multi_logloss: 0.404762\tvalid_1's multi_logloss: 0.404762\n",
            "[233]\ttraining's multi_logloss: 0.110078\ttraining's multi_logloss: 0.110078\tvalid_1's multi_logloss: 0.404755\tvalid_1's multi_logloss: 0.404755\n",
            "[234]\ttraining's multi_logloss: 0.110069\ttraining's multi_logloss: 0.110069\tvalid_1's multi_logloss: 0.404752\tvalid_1's multi_logloss: 0.404752\n",
            "[235]\ttraining's multi_logloss: 0.11006\ttraining's multi_logloss: 0.11006\tvalid_1's multi_logloss: 0.404743\tvalid_1's multi_logloss: 0.404743\n",
            "[236]\ttraining's multi_logloss: 0.110052\ttraining's multi_logloss: 0.110052\tvalid_1's multi_logloss: 0.404739\tvalid_1's multi_logloss: 0.404739\n",
            "[237]\ttraining's multi_logloss: 0.110043\ttraining's multi_logloss: 0.110043\tvalid_1's multi_logloss: 0.404732\tvalid_1's multi_logloss: 0.404732\n",
            "[238]\ttraining's multi_logloss: 0.110034\ttraining's multi_logloss: 0.110034\tvalid_1's multi_logloss: 0.40473\tvalid_1's multi_logloss: 0.40473\n",
            "[239]\ttraining's multi_logloss: 0.110027\ttraining's multi_logloss: 0.110027\tvalid_1's multi_logloss: 0.404725\tvalid_1's multi_logloss: 0.404725\n",
            "[240]\ttraining's multi_logloss: 0.110018\ttraining's multi_logloss: 0.110018\tvalid_1's multi_logloss: 0.404722\tvalid_1's multi_logloss: 0.404722\n",
            "[241]\ttraining's multi_logloss: 0.11001\ttraining's multi_logloss: 0.11001\tvalid_1's multi_logloss: 0.404716\tvalid_1's multi_logloss: 0.404716\n",
            "[242]\ttraining's multi_logloss: 0.109998\ttraining's multi_logloss: 0.109998\tvalid_1's multi_logloss: 0.40471\tvalid_1's multi_logloss: 0.40471\n",
            "[243]\ttraining's multi_logloss: 0.109989\ttraining's multi_logloss: 0.109989\tvalid_1's multi_logloss: 0.404706\tvalid_1's multi_logloss: 0.404706\n",
            "[244]\ttraining's multi_logloss: 0.109982\ttraining's multi_logloss: 0.109982\tvalid_1's multi_logloss: 0.404703\tvalid_1's multi_logloss: 0.404703\n",
            "[245]\ttraining's multi_logloss: 0.109972\ttraining's multi_logloss: 0.109972\tvalid_1's multi_logloss: 0.404702\tvalid_1's multi_logloss: 0.404702\n",
            "[246]\ttraining's multi_logloss: 0.109964\ttraining's multi_logloss: 0.109964\tvalid_1's multi_logloss: 0.404699\tvalid_1's multi_logloss: 0.404699\n",
            "[247]\ttraining's multi_logloss: 0.109956\ttraining's multi_logloss: 0.109956\tvalid_1's multi_logloss: 0.404694\tvalid_1's multi_logloss: 0.404694\n",
            "[248]\ttraining's multi_logloss: 0.109946\ttraining's multi_logloss: 0.109946\tvalid_1's multi_logloss: 0.404695\tvalid_1's multi_logloss: 0.404695\n",
            "[249]\ttraining's multi_logloss: 0.10994\ttraining's multi_logloss: 0.10994\tvalid_1's multi_logloss: 0.404694\tvalid_1's multi_logloss: 0.404694\n",
            "[250]\ttraining's multi_logloss: 0.109931\ttraining's multi_logloss: 0.109931\tvalid_1's multi_logloss: 0.404691\tvalid_1's multi_logloss: 0.404691\n",
            "[251]\ttraining's multi_logloss: 0.109925\ttraining's multi_logloss: 0.109925\tvalid_1's multi_logloss: 0.40469\tvalid_1's multi_logloss: 0.40469\n",
            "[252]\ttraining's multi_logloss: 0.109917\ttraining's multi_logloss: 0.109917\tvalid_1's multi_logloss: 0.404688\tvalid_1's multi_logloss: 0.404688\n",
            "[253]\ttraining's multi_logloss: 0.10991\ttraining's multi_logloss: 0.10991\tvalid_1's multi_logloss: 0.404688\tvalid_1's multi_logloss: 0.404688\n",
            "[254]\ttraining's multi_logloss: 0.1099\ttraining's multi_logloss: 0.1099\tvalid_1's multi_logloss: 0.404684\tvalid_1's multi_logloss: 0.404684\n",
            "[255]\ttraining's multi_logloss: 0.109893\ttraining's multi_logloss: 0.109893\tvalid_1's multi_logloss: 0.404684\tvalid_1's multi_logloss: 0.404684\n",
            "[256]\ttraining's multi_logloss: 0.109886\ttraining's multi_logloss: 0.109886\tvalid_1's multi_logloss: 0.404684\tvalid_1's multi_logloss: 0.404684\n",
            "[257]\ttraining's multi_logloss: 0.109881\ttraining's multi_logloss: 0.109881\tvalid_1's multi_logloss: 0.404685\tvalid_1's multi_logloss: 0.404685\n",
            "[258]\ttraining's multi_logloss: 0.109876\ttraining's multi_logloss: 0.109876\tvalid_1's multi_logloss: 0.404684\tvalid_1's multi_logloss: 0.404684\n",
            "[259]\ttraining's multi_logloss: 0.109872\ttraining's multi_logloss: 0.109872\tvalid_1's multi_logloss: 0.404685\tvalid_1's multi_logloss: 0.404685\n",
            "[260]\ttraining's multi_logloss: 0.109868\ttraining's multi_logloss: 0.109868\tvalid_1's multi_logloss: 0.404685\tvalid_1's multi_logloss: 0.404685\n",
            "[261]\ttraining's multi_logloss: 0.109864\ttraining's multi_logloss: 0.109864\tvalid_1's multi_logloss: 0.404683\tvalid_1's multi_logloss: 0.404683\n",
            "[262]\ttraining's multi_logloss: 0.109859\ttraining's multi_logloss: 0.109859\tvalid_1's multi_logloss: 0.404681\tvalid_1's multi_logloss: 0.404681\n",
            "[263]\ttraining's multi_logloss: 0.109855\ttraining's multi_logloss: 0.109855\tvalid_1's multi_logloss: 0.404681\tvalid_1's multi_logloss: 0.404681\n",
            "[264]\ttraining's multi_logloss: 0.109852\ttraining's multi_logloss: 0.109852\tvalid_1's multi_logloss: 0.404682\tvalid_1's multi_logloss: 0.404682\n",
            "[265]\ttraining's multi_logloss: 0.109849\ttraining's multi_logloss: 0.109849\tvalid_1's multi_logloss: 0.404683\tvalid_1's multi_logloss: 0.404683\n",
            "[266]\ttraining's multi_logloss: 0.109845\ttraining's multi_logloss: 0.109845\tvalid_1's multi_logloss: 0.404684\tvalid_1's multi_logloss: 0.404684\n",
            "[267]\ttraining's multi_logloss: 0.109842\ttraining's multi_logloss: 0.109842\tvalid_1's multi_logloss: 0.404682\tvalid_1's multi_logloss: 0.404682\n",
            "[268]\ttraining's multi_logloss: 0.109838\ttraining's multi_logloss: 0.109838\tvalid_1's multi_logloss: 0.404681\tvalid_1's multi_logloss: 0.404681\n",
            "[269]\ttraining's multi_logloss: 0.109836\ttraining's multi_logloss: 0.109836\tvalid_1's multi_logloss: 0.404683\tvalid_1's multi_logloss: 0.404683\n",
            "[270]\ttraining's multi_logloss: 0.109833\ttraining's multi_logloss: 0.109833\tvalid_1's multi_logloss: 0.404685\tvalid_1's multi_logloss: 0.404685\n",
            "[271]\ttraining's multi_logloss: 0.10983\ttraining's multi_logloss: 0.10983\tvalid_1's multi_logloss: 0.404685\tvalid_1's multi_logloss: 0.404685\n",
            "[272]\ttraining's multi_logloss: 0.109828\ttraining's multi_logloss: 0.109828\tvalid_1's multi_logloss: 0.404684\tvalid_1's multi_logloss: 0.404684\n",
            "[273]\ttraining's multi_logloss: 0.109825\ttraining's multi_logloss: 0.109825\tvalid_1's multi_logloss: 0.404685\tvalid_1's multi_logloss: 0.404685\n",
            "[274]\ttraining's multi_logloss: 0.109821\ttraining's multi_logloss: 0.109821\tvalid_1's multi_logloss: 0.404686\tvalid_1's multi_logloss: 0.404686\n",
            "[275]\ttraining's multi_logloss: 0.109818\ttraining's multi_logloss: 0.109818\tvalid_1's multi_logloss: 0.404686\tvalid_1's multi_logloss: 0.404686\n",
            "[276]\ttraining's multi_logloss: 0.109814\ttraining's multi_logloss: 0.109814\tvalid_1's multi_logloss: 0.404686\tvalid_1's multi_logloss: 0.404686\n",
            "[277]\ttraining's multi_logloss: 0.109813\ttraining's multi_logloss: 0.109813\tvalid_1's multi_logloss: 0.404687\tvalid_1's multi_logloss: 0.404687\n",
            "[278]\ttraining's multi_logloss: 0.109811\ttraining's multi_logloss: 0.109811\tvalid_1's multi_logloss: 0.404686\tvalid_1's multi_logloss: 0.404686\n",
            "[279]\ttraining's multi_logloss: 0.109809\ttraining's multi_logloss: 0.109809\tvalid_1's multi_logloss: 0.404688\tvalid_1's multi_logloss: 0.404688\n",
            "[280]\ttraining's multi_logloss: 0.109808\ttraining's multi_logloss: 0.109808\tvalid_1's multi_logloss: 0.404688\tvalid_1's multi_logloss: 0.404688\n",
            "[281]\ttraining's multi_logloss: 0.109806\ttraining's multi_logloss: 0.109806\tvalid_1's multi_logloss: 0.404688\tvalid_1's multi_logloss: 0.404688\n",
            "[282]\ttraining's multi_logloss: 0.109805\ttraining's multi_logloss: 0.109805\tvalid_1's multi_logloss: 0.40469\tvalid_1's multi_logloss: 0.40469\n",
            "[283]\ttraining's multi_logloss: 0.109803\ttraining's multi_logloss: 0.109803\tvalid_1's multi_logloss: 0.404692\tvalid_1's multi_logloss: 0.404692\n",
            "Early stopping, best iteration is:\n",
            "[263]\ttraining's multi_logloss: 0.109855\ttraining's multi_logloss: 0.109855\tvalid_1's multi_logloss: 0.404681\tvalid_1's multi_logloss: 0.404681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
              "               importance_type='split', learning_rate=0.05, max_depth=-1,\n",
              "               min_child_samples=17, min_child_weight=0.005, min_data_in_leaf=5,\n",
              "               min_split_gain=0.34, n_estimators=10000, n_jobs=-1,\n",
              "               num_leaves=500, objective='multiclass', random_state=123,\n",
              "               reg_alpha=0.01, reg_lambda=0.01, silent=True, subsample=1.0,\n",
              "               subsample_for_bin=250000, subsample_freq=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4WH4TRPYqAt",
        "colab_type": "text"
      },
      "source": [
        "## LGBM voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiaK7FvyYs6p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "#############\n",
        "\n",
        "lgbm_wrapper1 = LGBMClassifier(boosting_type='gbdt', num_leaves=250, max_depth=-1, learning_rate=0.005, min_data_in_leaf = 5,\n",
        "                             n_estimators=10000, subsample_for_bin=250000, objective='multiclass', min_split_gain=0.34, reg_alpha = 0.01, reg_lambda = 0.01,\n",
        "                             min_child_weight=0.005, min_child_samples=17,n_jobs=-1, random_state=123)\n",
        "###############\n",
        "\n",
        "lgbm_wrapper2 = LGBMClassifier(boosting_type='gbdt', num_leaves=300, max_depth=-1, learning_rate=0.005, min_data_in_leaf = 5,\n",
        "                             n_estimators=10000, subsample_for_bin=250000, objective='multiclass', min_split_gain=0.34, reg_alpha = 0.01, reg_lambda = 0.01,\n",
        "                             min_child_weight=0.005, min_child_samples=17,n_jobs=-1, random_state=123)\n",
        "#################\n",
        "\n",
        "lgbm_wrapper3 = LGBMClassifier(boosting_type='gbdt', num_leaves=350, max_depth=-1, learning_rate=0.005, min_data_in_leaf = 5,\n",
        "                             n_estimators=10000, subsample_for_bin=250000, objective='multiclass', min_split_gain=0.34, reg_alpha = 0.01, reg_lambda = 0.01,\n",
        "                             min_child_weight=0.005, min_child_samples=17,n_jobs=-1, random_state=123)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnFCyfZKclcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "3e42f805-45bb-444b-e2b1-5d17df168cd9"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lgb1', lgbm_wrapper1), ('lgb2', lgbm_wrapper2), ('lgb3', lgbm_wrapper3)],\n",
        "    voting='soft')\n",
        "\n",
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VotingClassifier(estimators=[('lgb1',\n",
              "                              LGBMClassifier(boosting_type='gbdt',\n",
              "                                             class_weight=None,\n",
              "                                             colsample_bytree=1.0,\n",
              "                                             importance_type='split',\n",
              "                                             learning_rate=0.005, max_depth=-1,\n",
              "                                             min_child_samples=17,\n",
              "                                             min_child_weight=0.005,\n",
              "                                             min_data_in_leaf=5,\n",
              "                                             min_split_gain=0.34,\n",
              "                                             n_estimators=10000, n_jobs=-1,\n",
              "                                             num_leaves=250,\n",
              "                                             objective='multiclass',\n",
              "                                             random_state=123, reg_alpha=0.01...\n",
              "                                             learning_rate=0.005, max_depth=-1,\n",
              "                                             min_child_samples=17,\n",
              "                                             min_child_weight=0.005,\n",
              "                                             min_data_in_leaf=5,\n",
              "                                             min_split_gain=0.34,\n",
              "                                             n_estimators=10000, n_jobs=-1,\n",
              "                                             num_leaves=350,\n",
              "                                             objective='multiclass',\n",
              "                                             random_state=123, reg_alpha=0.01,\n",
              "                                             reg_lambda=0.01, silent=True,\n",
              "                                             subsample=1.0,\n",
              "                                             subsample_for_bin=250000,\n",
              "                                             subsample_freq=0))],\n",
              "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
              "                 weights=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luVJBgz03K3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6538d855-5a9e-44ae-d2dd-37951f30e52b"
      },
      "source": [
        "yk_grd_probs = voting_clf.predict_proba(X_valid)\n",
        "print(log_loss(y_valid, yk_grd_probs))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.39204467979189705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbq02KfydXmm",
        "colab_type": "text"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VHa_oNg-vum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = voting_clf.predict_proba(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z84RvaqG_m0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.DataFrame(preds)\n",
        "submission = pd.concat([test_id, submission], axis=1)\n",
        "submission.columns=['id', 'STAR_WHITE_DWARF', 'STAR_CATY_VAR', 'STAR_BROWN_DWARF',\n",
        "       'SERENDIPITY_RED', 'REDDEN_STD', 'STAR_BHB', 'GALAXY',\n",
        "       'SERENDIPITY_DISTANT', 'QSO', 'SKY', 'STAR_RED_DWARF', 'ROSAT_D',\n",
        "       'STAR_PN', 'SERENDIPITY_FIRST', 'STAR_CARBON', 'SPECTROPHOTO_STD',\n",
        "       'STAR_SUB_DWARF', 'SERENDIPITY_MANUAL', 'SERENDIPITY_BLUE']\n",
        "\n",
        "submission.to_csv('/content/drive/My Drive/Colab Notebooks/data/데이콘/천체 유형 분류/submission_lgbm_Voting.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}